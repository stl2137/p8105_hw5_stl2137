---
title: "p8105_hw5_stl2137"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(patchwork)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

```{r}
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```

```{r}
fill_iris <- function(x) {
  if(is.character(x)) {
    replace(x, is.na(x), "virginica")
  } else if(is.numeric(x)) {
    replace(x, is.na(x), mean(x, na.rm = TRUE))
  }
}

filled_iris <- map_df(.x = iris_with_missing, ~fill_iris(.x))
```

## Problem 2

```{r}
long_study_tidy <- list.files(path = "./data/", pattern = "*.csv", full.names = TRUE) %>% 
  map_df(~read_csv(.)) %>% 
  mutate(
    subject_id = list.files("./data") %>% str_extract('.*(?=\\.csv$)')
  ) %>% 
  separate(subject_id, into = c("arm", "subject_number_id"), sep = "_") %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation"
  ) %>% 
  mutate(
    observation = as.numeric(observation)
  ) 
```

* The experimental group over the 8 weeks have a trend of increasing observations compared with the control group observations over the 8 weeks. 

```{r}
long_study_tidy %>% 
  group_by(arm, subject_number_id) %>% 
  ggplot(aes(x = week, y = observation, group = arm, color = arm)) +
  geom_path() 
```

## Problem 3

```{r}

sim_regression = function(beta1) {
  
  sim_data = tibble(
    x = rnorm(30, mean = 0, sd = 1),
    y = 2 + beta1 * x + rnorm(30, 0, sqrt(50))
  )
  
  ls_fit = lm(y ~ x, data = sim_data)
  
  tibble(
    term = pull(broom::tidy(ls_fit), term),
    beta1_hat = pull(broom::tidy(ls_fit), estimate),
    p_value = pull(broom::tidy(ls_fit), p.value)
  )
}

sim_regression(0)

sim_results_0 <- 
  rerun(10000, sim_regression(0)) %>% 
  bind_rows() %>% 
  filter(term %in% "x")
```

### REMEMBER TO CHANGE BACK TO 10000 B/C IT TAKES TOO LONG TO RUN ### 

```{r}
output = vector("list", length = 7)

for (i in 1:7) {
  
  output[[i]] = rerun(10000, sim_regression(i-1)) %>% 
    bind_rows() %>% 
    mutate(
      beta1_group = i-1
    )
}

sim_results_0_6 <- 
  output %>% 
  bind_rows() %>% 
  filter(term %in% "x")
```

```{r}
sim_results_0_6 %>% 
  filter(p_value < 0.05) %>% 
  group_by(beta1_group) %>% 
  summarise(proportion = n()/10000) %>% 
  ggplot(aes(x = beta1_group, y = proportion)) +
  geom_point() +
  labs(
    title = "Association between Effect Size and Power",
    x = "Value of β1",
    y = "Power of test"
  ) +
  scale_x_continuous(breaks = 0:6)
```

* As the value of β1 increased, the power of the test also increased. 

```{r}
beta_hat_vs_true <- sim_results_0_6 %>% 
  group_by(beta1_group) %>% 
  summarise(
    avg_est_beta1 = mean(beta1_hat)) %>% 
  ggplot(aes(x = beta1_group, y = avg_est_beta1)) +
  geom_point() +
  labs(
    title = "True value of β1 vs. Mean estimate of β̂ 1  ",
    x = "True Value of β1",
    y = "Average Estimate of β1"
  ) +
  scale_x_continuous(breaks = 0:6) +
  scale_y_continuous(breaks = 0:6)

null_rejected_beta_hat_vs_true <- sim_results_0_6 %>% 
  filter(p_value < 0.05) %>% 
  group_by(beta1_group) %>% 
  summarise(
    avg_est_beta1 = mean(beta1_hat)) %>% 
  ggplot(aes(x = beta1_group, y = avg_est_beta1)) +
  geom_point() +
  labs(
    title = "True value of β1 vs. Mean estimate of β̂ 1 where null is rejected ",
    x = "True Value of β1",
    y = "Average Estimate of β1 where null is rejected"
  ) +
  scale_x_continuous(breaks = 0:6) +
  scale_y_continuous(breaks = 0:6)

beta_hat_vs_true + null_rejected_beta_hat_vs_true
```

Is the sample average of β̂ 1 across tests for which the null is rejected approximately equal to the true value of β1? Why or why not?

The sample average of beta hat1 across tests for which the null is rejected is not equal to the true values of beta1. Because we filter for beta hat values where the null is rejected, the average of these beta hats is skewed since the lower true values of beta1 have lower power as shown previously. Lower power increases the probability of committing a type 2 error. 